jobconfig = {
    'gpu': 1,
    'debug': False,
    'model': 'BiLSTM_withAttention_residual_late_fusion',
    'datasetName': 'swbd_prosody_ms_word_level',
    'expPath': '',
    'basedirName': '/s1/vzayats/experiments/disfl',
    'modelName': None,
    'expdate': '10_12_2018',
    'valid_prosody_only': True,
    'labels': 'full', #'base','full','full_with_s'
    'outlayer': 'crf', # 'softmax', 'crf'
    'train_embeddings': False,
    'text_only_preds': False,#True,
    'feats_to_include': ['POS'] + ['feat%d' % i for i in [1,2,3,4]],
    'window_size': 10,
    'attention': 'single_multimodal',
    'attention_type': 'sim',
    'attention_activation': 'tanh',
    'num_filters': 16,
    'filter_shapes': [(3,3),(5,3),(3,1),(5,1),(1,1)],
    'num_feat': 20,
    'num_heads': 8,
    #'clipnorm': [0, 0.5, 1, 10, 100, 1000],
    'conv': False,
    'conv_nonlin': True,
    'attention_maxpool': False,
    'num_concurrent_jobs': 1,
    'posDimensions': 5,
    'weights': [1,1,1,1],
    'text_prosody_loss': 'mean_squared_error',
    'diff_loss': False,
    'lm': False,
    'lr': 0.0005,
    'phone_att': False,
    'second_layer': 'lstm',
    'f_lm_pickle': 'swbd_cleaned_lm_forward.pkl',
    'b_lm_pickle': 'swbd_cleaned_lm_backward.pkl',
    'phone_feats': [('total_phone_durations',1),('conv_phone_feats',19)],
    'phoneEmbeddingsSize': 10,
    'stressEmbeddingsSize': 5,
    'phoneLSTMSize': 50,
    'prosody': True,
    'prosody_conv': True,
    'prosody_feats': [('pause_before',1)],
    'pretrain_text': False,
    'pretrained_text_only_model': '0.8795_0.8780_4_500.h5',
    'alpha': [0.05, 0.1, 0.2, 0.3, 0.5],
    }
